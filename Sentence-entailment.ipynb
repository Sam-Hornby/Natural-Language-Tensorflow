{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n"
     ]
    }
   ],
   "source": [
    "glove_dimension = 50\n",
    "glove_dimension = 50\n",
    "def glove_dictionary(file_path, glove_d):\n",
    "    # select appropriate glove file for glove dimension given\n",
    "    glove_vectors = open(file_path)    \n",
    "\n",
    "    length = 0\n",
    "    vocab = []\n",
    "    dictionary = {}\n",
    "    for line in glove_vectors:\n",
    "        item = line.split(\" \")\n",
    "        vocab.append(item[0])\n",
    "        word_vector = [float(n) for n in item[1:]]\n",
    "        dictionary[item[0]] = word_vector\n",
    "        length = length + 1\n",
    "        #if length > 10:\n",
    "         #   break\n",
    "    \n",
    "    glove_vectors.close()\n",
    "    return dictionary\n",
    "\n",
    "word_dictionary = glove_dictionary(\"glove.6B/glove.6B.50d.txt\", glove_dimension)\n",
    "\n",
    "\n",
    "\n",
    "def glove(word, dictionary, glove_d):\n",
    "    if word in dictionary:\n",
    "        return  dictionary[word]\n",
    "    else:\n",
    "        #values taken from paper\n",
    "        return np.random.uniform(-0.05, 0.05, glove_d).tolist()\n",
    "\n",
    "print(glove(\"the\", word_dictionary, glove_dimension))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(file, max_num):\n",
    "    # I reccomend that the max_num is kept very small as my padding function is very slow \n",
    "    i = 0\n",
    "    data = open(file)\n",
    "    SNLI_sentences = []\n",
    "    for pair in data:\n",
    "        pair = json.loads(pair)          # pair will be string so need json.loads to turn it into a dictionary\n",
    "        if pair[\"gold_label\"] == \"-\":\n",
    "            continue                            #get rid of useless results\n",
    "        SNLI_sentences.append([pair[\"sentence1\"].lower(), pair[\"sentence2\"].lower(), pair[\"gold_label\"]])\n",
    "\n",
    "        i = i + 1\n",
    "        if i > max_num:\n",
    "            break\n",
    "    # close open file to free up memory\n",
    "    data.close()\n",
    "\n",
    "    return SNLI_sentences\n",
    "\n",
    "# sentence_list is list of lists where the inner lists are [Premise, Hypothesis, label]\n",
    "sentence_list = get_data(\"snli_1.0/snli_1.0_train.jsonl\", 1000000)\n",
    "test_list = get_data(\"snli_1.0/snli_1.0_test.jsonl\", 500000)\n",
    "\n",
    "\n",
    "\n",
    "#print(sentence_list)\n",
    "\n",
    "def sentence_vectoriser(sentence):\n",
    "    sentence_array = [glove(item, word_dictionary, glove_dimension) for item in sentence.split(\" \")]\n",
    "    return sentence_array\n",
    "\n",
    "def OHencoder(tensor):\n",
    "    # manually encoding labels. if there were more classes then could use SKlearn's one hot encoder but for 3 classes\n",
    "    # this seems like overkill\n",
    "    #label = tensor[i][2]\n",
    "    enc_dict = {\"entailment\": [1,0,0], \"neutral\":[0,1,0], \"contradiction\":[0,0,1]}\n",
    "    #loop backwards here so that when deleting items it doesn't skip forward\n",
    "    #or i in range(len(tensor)):\n",
    "    n = len(tensor)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        label = tensor[-i][2]\n",
    "        try:\n",
    "            tensor[-i][2] = enc_dict[label]\n",
    "        except KeyError:      \n",
    "            del tensor[-i]\n",
    "            n = n - 1\n",
    "        except TypeError:\n",
    "            del tensor[-i]\n",
    "            n = n - 1\n",
    "        i = i + 1\n",
    "        \"\"\" label in enc_dict:\n",
    "            tensor[-i][2] = enc_dict[label]\n",
    "            \n",
    "        else:\n",
    "            del tensor[-i]\"\"\"\n",
    "    return tensor\n",
    "            \n",
    "        \n",
    "\n",
    "def pad_sentence_lengths(tensor):\n",
    "    max_length = 0\n",
    "    for sen_arr in tensor:\n",
    "        max_length = max([max_length, len(sen_arr)])\n",
    "    \n",
    "    padding = [0 for a in range(glove_dimension)]\n",
    "    \n",
    "    for i in range(len(tensor)):\n",
    "        #tensor[i] = np.lib.pad(tensor[i], (0, max_length - len(tensor[i])), \"constant\", constant_values = 0)\n",
    "        for m in range((max_length - len(tensor[i]))):\n",
    "            tensor[i].append(padding)\n",
    "\n",
    "\n",
    "    return tensor, max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This step takes a long time due to the bad padding function\n",
    "\n",
    "OH_sentence_list = OHencoder(sentence_list)\n",
    "#train_premises, max_plength = pad_sentence_lengths([sentence_vectoriser(i[0]) for i in OH_sentence_list])\n",
    "#train_hypothesis, max_hlength = pad_sentence_lengths([sentence_vectoriser(i[1]) for i in OH_sentence_list])\n",
    "train_labels = [i[2] for i in OH_sentence_list]\n",
    "\n",
    "tr_prem = [sentence_vectoriser(i[0]) for i in OH_sentence_list]\n",
    "tr_hyp = [sentence_vectoriser(i[1]) for i in OH_sentence_list]\n",
    "\n",
    "train_data_length = len(tr_prem)\n",
    "print(train_data_length)\n",
    "\n",
    "OH_test_list = OHencoder(test_list)\n",
    "#test_premises, max_tplength = pad_sentence_lengths([sentence_vectoriser(i[0]) for i in OH_test_list])\n",
    "#test_hypothesis, max_thlength = pad_sentence_lengths([sentence_vectoriser(i[1]) for i in OH_test_list])\n",
    "test_labels = [i[2] for i in OH_test_list]\n",
    "\n",
    "te_prem = [sentence_vectoriser(i[0]) for i in OH_test_list]\n",
    "te_hyp = [sentence_vectoriser(i[1]) for i in OH_test_list]\n",
    "\n",
    "test_data_length = len(te_prem)\n",
    "print(test_data_length)\n",
    "\n",
    "unpadded_premise_data = tr_prem + te_prem\n",
    "unpadded_hypothesis_data = tr_hyp + te_hyp\n",
    "d = train_labels + test_labels\n",
    "data_labels = np.array(d)\n",
    "print(len(unpadded_premise_data))\n",
    "premise_data, max_plength = pad_sentence_lengths(unpadded_premise_data)\n",
    "hypothesis_data, max_hlength = pad_sentence_lengths(unpadded_hypothesis_data)\n",
    "\n",
    "print(max_plength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n"
     ]
    }
   ],
   "source": [
    "# I had to define batch size here to be able to run the for loop in the attention function. \n",
    "# There is a way that this can be avoided using tf.scan but I ran out of time trying to work out how to do this.\n",
    "# means when calculating the score of the test set I will have to do this in batches.\n",
    "batch_size = 50\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def length(sequence):\n",
    "    used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "    length = tf.reduce_sum(used, reduction_indices=1)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    return length\n",
    "    \n",
    "\n",
    "\n",
    "# I havent bothered to implement the linear layer here\n",
    "LSTM_size = glove_dimension\n",
    "\n",
    "#input data as 3d tensors\n",
    "input_premises = tf.placeholder(tf.float32, shape = [batch_size, max_plength, glove_dimension])\n",
    "input_hypothesis = tf.placeholder(tf.float32, shape = [batch_size, max_hlength, glove_dimension])\n",
    "\n",
    "#must change scope so that the 2 lstms when run don't have the same variable name\n",
    "with tf.variable_scope(\"first_lstm\"):\n",
    "    LSTM_cell = tf.nn.rnn_cell.BasicLSTMCell(LSTM_size, forget_bias = 1.0, state_is_tuple=True)\n",
    "    p_out, p_state = tf.nn.dynamic_rnn(LSTM_cell, input_premises, dtype=tf.float32, sequence_length=length(input_premises))\n",
    "\n",
    "#now want p_state tensor to iniatlize the hypothesis lstm\n",
    "with tf.variable_scope(\"second_lstm\"):\n",
    "    LSTM_cell = tf.nn.rnn_cell.BasicLSTMCell(LSTM_size, forget_bias = 1.0, state_is_tuple=True)\n",
    "    h_out, h_state = tf.nn.dynamic_rnn(LSTM_cell, input_hypothesis, initial_state = p_state, dtype=tf.float32, sequence_length=length(input_hypothesis))\n",
    "\n",
    "\n",
    "# define variables for the attention function\n",
    "Wy = tf.Variable(tf.truncated_normal([LSTM_size, LSTM_size]))\n",
    "Wh = tf.Variable(tf.truncated_normal([LSTM_size, LSTM_size]))\n",
    "Wv = tf.Variable(tf.truncated_normal([LSTM_size, 1]))\n",
    "Wp = tf.Variable(tf.truncated_normal([LSTM_size, LSTM_size]))\n",
    "Wx = tf.Variable(tf.truncated_normal([LSTM_size, LSTM_size]))\n",
    "\n",
    "# h_out will be tensor with shape [batch size, max time, cell.output_size]. make max time first axis using perm\n",
    "h_out = tf.transpose(h_out, perm = [1, 0 , 2])\n",
    "last = tf.gather(h_out, int(h_out.get_shape()[0]) - 1)\n",
    "\n",
    "def attention(H, Y, WY, WH, w, WP, WX):\n",
    "    b = Y.get_shape()[0]\n",
    "    seq_l = Y.get_shape()[1]\n",
    "    #eL = tf.ones([seq_l, 1])\n",
    "    gather_list = [0 for i in range(seq_l)]\n",
    "    \n",
    "    \n",
    "    for i in range(b):\n",
    "        Y_i = tf.squeeze(tf.gather(Y, [i]))\n",
    "        H_i = tf.gather(H, [i])\n",
    "        #use gather instead of outer product to create the tensor we want\n",
    "        outer = tf.gather(H_i, gather_list)\n",
    "        \n",
    "        M = tf.tanh(tf.matmul(Y_i, WY) + tf.matmul(outer, WH))\n",
    "        alpha = tf.nn.softmax(tf.matmul(M, w))\n",
    "        \n",
    "        r = tf.matmul(alpha, Y_i, transpose_a=True)\n",
    "        \n",
    "        h_star = tf.tanh(tf.matmul(r, WP) + tf.matmul(H_i, WX))\n",
    "        \n",
    "        if i == 0:\n",
    "            H_star = h_star\n",
    "        else:\n",
    "            H_star = tf.concat(0, [H_star, h_star])\n",
    "    \n",
    "    print(H_star.get_shape())\n",
    "    return H_star\n",
    "        \n",
    "    \n",
    "Att = attention(last, p_out, Wy, Wh, Wv, Wp, Wx)\n",
    "\n",
    "\n",
    "#final linear layer to get outputs in shape (batch size, 3).\n",
    "weight = tf.Variable(tf.truncated_normal([LSTM_size, 3]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[3]))\n",
    "y_ = tf.placeholder(tf.float32, shape = [batch_size, 3])\n",
    "\n",
    "\n",
    "y = tf.matmul(Att, weight) + bias\n",
    "#softmax to get predictions\n",
    "cross_entropy = -tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "\n",
    "#minimise using the ADAM optimiser\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "minimize = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# now work out how many the model gets right\n",
    "correct = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "num_correct = tf.reduce_sum(tf.cast(correct, tf.float32))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 29.940119760479043 percent\n"
     ]
    }
   ],
   "source": [
    "init_op = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "no_of_batches = int(float(train_data_length)/batch_size)\n",
    "ptr = 0\n",
    "for j in range(no_of_batches):\n",
    "    inp_p, inp_h, out = premise_data[ptr:ptr+batch_size], hypothesis_data[ptr:ptr+batch_size], data_labels[ptr:ptr+batch_size]\n",
    "    ptr += batch_size\n",
    "    sess.run(minimize,{input_premises: inp_p, input_hypothesis: inp_h, y_: out})\n",
    "\n",
    "right = sess.run(num_correct, {input_premises: inp_p, input_hypothesis: inp_h, y_: out})\n",
    "pt = train_data_length\n",
    "cumu_right = 0\n",
    "no_test_batches = int(test_data_length/batch_size)\n",
    "for l in range(no_test_batches):\n",
    "    test_p, test_h, test_l = premise_data[pt:pt+batch_size], hypothesis_data[pt:pt+batch_size], data_labels[pt:pt+batch_size]\n",
    "    right = sess.run(num_correct, {input_premises: test_p, input_hypothesis: test_h, y_: test_l})\n",
    "    cumu_right = cumu_right + right\n",
    "accuracy = 100 * cumu_right/test_data_length\n",
    "print(\"accuracy is {0} percent\".format(accuracy))\n",
    "\n",
    "# To get the same results as the paper remember to add the linear layer. \n",
    "# To increase speed put the sentences into buckets rather than one large data set \n",
    "# \n",
    "# \n",
    "# \n",
    "# \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
